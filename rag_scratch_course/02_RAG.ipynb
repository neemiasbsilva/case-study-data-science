{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbCRrI4SZOEvv/Bezlr8f3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG From scratch: Query Transformations\n",
        "\n",
        "## Purpose\n",
        "\n",
        "This notebook has the purpose to explain query transformation. A simple definition, query transformation are a set of approaches focused on re-writing and/or modifying questions for retrieval.\n",
        "\n",
        "- Multi-Query\n",
        "    - [Multi-Query Docs](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever)\n",
        "- RAG-Fusion\n",
        "    - [RAG-Fusion Docs](https://github.com/langchain-ai/langchain/blob/master/cookbook/rag_fusion.ipynb?ref=blog.langchain.dev)\n",
        "    - [Forget RAG the future is RAG Fusion](https://towardsdatascience.com/forget-rag-the-future-is-rag-fusion-1147298d8ad1)\n",
        "- Decomposition\n",
        "    - Asnwer recursively\n",
        "        - [LEAST-TO-MOST PROMPTING ENABLES COMPLEX REASONING IN LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2205.10625)\n",
        "        - [Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions](https://arxiv.org/abs/2212.10509.pdf)\n",
        "    - Answer Individualy\n",
        "- Step-back\n",
        "    - [Take a Step Back: Evoking Reasoning via Abstraction in Large Language Models](https://arxiv.org/pdf/2310.06117)\n",
        "- HyDE\n",
        "    - [Improve Document indexing with HyDE](https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb)\n",
        "    - [Precise Zero-Shot Dense Retrieval without Relevance Labels](https://github.com/langchain-ai/langchain/blob/master/cookbook/hypothetical_document_embeddings.ipynb)"
      ],
      "metadata": {
        "id": "0Q8UGA9WOS2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "\n",
        "`(1) Packages`"
      ],
      "metadata": {
        "id": "MnDE0P74O4SQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_xnmTeuOI3E",
        "outputId": "db40bb3c-58c6-4dc6-b86d-58190eaae404"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.9/2.5 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m606.2/606.2 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.1/442.1 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community langchain_openai langchainhub langchain -q\n",
        "!pip install tiktoken chromadb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`(2) LangSmith`"
      ],
      "metadata": {
        "id": "3wpZ4NsFPOQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\""
      ],
      "metadata": {
        "id": "MeR4ONLTrQ1P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "if api_key:\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = api_key\n",
        "else:\n",
        "    api_key = input(\"Enter your API key: \")\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "9Vk7sMmz627L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`(3) API Keys`"
      ],
      "metadata": {
        "id": "WoAoJKVysTvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "if api_key:\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "else:\n",
        "    api_key = input(\"Enter your API key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "avf4vmga7S0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi Query"
      ],
      "metadata": {
        "id": "tVEuB8TTQWkc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indexing"
      ],
      "metadata": {
        "id": "kpqqFhj4RMzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=100,\n",
        ")\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Index\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=OpenAIEmbeddings(),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "HWgypbc1QRLS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt"
      ],
      "metadata": {
        "id": "aEpUJVYISYut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Multi Query: Different Perspectives\n",
        "template = \"\"\"You are AI language model assistant. Your task is to generate five\n",
        "different versions of the given user question to retrieve relevant documents from\n",
        "a vector database. By generating multiple perspectives on the user question,\n",
        "your goal is to help the user overcome some of the limitations of the\n",
        "distance-based similarity search. Provide these alternative questions separeted\n",
        "by new lines. Original question: {question}\n",
        "\"\"\"\n",
        "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_perspectives\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "Mzaxt25SSOMt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def get_unique_union(documents: list[list]):\n",
        "    \"\"\"Unique union of retrieved docs\"\"\"\n",
        "    # Flatten list of lists, and convert each Document to string\n",
        "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
        "\n",
        "    # Get unique documents\n",
        "    unique_docs = list(set(flattened_docs))\n",
        "\n",
        "    # Convert back to Document\n",
        "    unique_docs = [loads(doc) for doc in unique_docs]\n",
        "\n",
        "    return unique_docs\n",
        "\n",
        "# Retrieve\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
        "docs = retrieval_chain.invoke({\"question\": question})\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04HXgKdKTpNf",
        "outputId": "d47195d8-06f7-4f64-c6d1-d756ca4edd54"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "llm = ChatOpenAI(temperature=0)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\n",
        "        \"context\": retrieval_chain,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "bhxCM7w6Ug7H",
        "outputId": "95f7c34f-aa62-46cc-c5b5-f33a84313d4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals using methods such as simple prompting, task-specific instructions, and human inputs. This allows the agent to effectively plan and navigate through complex tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# RAG-Fusion: Related\n",
        "template = \"\"\"You are helpful assistant that generates multiple search queries\n",
        "based on a simple input query. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (4 queries):\n",
        "\"\"\"\n",
        "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "MtBDpt33VRf0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_queries = (\n",
        "    prompt_rag_fusion\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")"
      ],
      "metadata": {
        "id": "oICJdGllW7p-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.load import dumps, loads\n",
        "\n",
        "def reciprocal_rank_fusion(results: list[list], k=60):\n",
        "    \"\"\"Reciprocal_rank_fusion that takes multiple lists of ranked documents\n",
        "    and an optional parameter k used in the RRF formula\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize a dictionary to hold fused scores for each unique document\n",
        "    fused_scores = {}\n",
        "\n",
        "    # Iterate through each list of ranked documents\n",
        "    for docs in results:\n",
        "        # Iterate through each document in the list, with its rank (position\n",
        "        # in the list)\n",
        "        for rank, doc in enumerate(docs):\n",
        "            # Conver the document to a string format to use as a key (assumes\n",
        "            # documents can be serialized to JSON)\n",
        "            doc_str = dumps(doc)\n",
        "            # If the document is not yet in the fused_scores dictionary, add it\n",
        "            # with an initial score of 0\n",
        "            if doc_str not in fused_scores:\n",
        "                fused_scores[doc_str] = 0\n",
        "            # Retrieve the current score of the document, if any\n",
        "            previous_score = fused_scores[doc_str]\n",
        "            # Update the score of the document using th RRF formula:\n",
        "            # 1 / (rank + k)\n",
        "            fused_scores[doc_str] += 1 / (rank + k)\n",
        "\n",
        "    # Sort the documents based on their fused scores in desceding order to get\n",
        "    # the final reranked results\n",
        "    reranked_results = [\n",
        "        (loads(doc), score)\n",
        "        for doc, score in sorted(\n",
        "            fused_scores.items(),\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True,\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Return the reranked results as a list of tuples, each containing the\n",
        "    # document and its fused score\n",
        "    return reranked_results\n",
        "\n",
        "retrieval_chain_rag_fusion = (\n",
        "    generate_queries\n",
        "    | retriever.map()\n",
        "    | reciprocal_rank_fusion\n",
        ")\n",
        "\n",
        "docs = retrieval_chain_rag_fusion.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtobSVWwXIFd",
        "outputId": "0d12ec57-c928-4c2f-c685-b0f3bbc05919"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    {\n",
        "        \"context\": retrieval_chain_rag_fusion,\n",
        "        \"question\": itemgetter(\"question\")\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "WGwfhnbZZnbj",
        "outputId": "83da2526-80c9-4628-ff1e-9b049bade4a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks. This can be achieved through simple prompting, task-specific instructions, or with human inputs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decomposition"
      ],
      "metadata": {
        "id": "K7kj910UaNBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Decomposition\n",
        "template = \"\"\"You are a helpful assistant that generates multiple sub-questions. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that\n",
        "can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (3 queries):\n",
        "\"\"\"\n",
        "prompt_decomposition = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "VECErvYUaGXl"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "# Chain\n",
        "generate_queries_decomposition = (\n",
        "    prompt_decomposition\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    | (lambda x: x.split(\"\\n\"))\n",
        ")\n",
        "\n",
        "# Run\n",
        "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
        "questions = generate_queries_decomposition.invoke({\"question\": question})"
      ],
      "metadata": {
        "id": "GScBO5GAa2Bw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EUBUEv5be00",
        "outputId": "b2e9f76d-13c5-4c2a-be12-29b8571857fa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1. What is LLM technology and how is it used in autonomous agent systems?',\n",
              " '2. What are the key components of an autonomous agent system?',\n",
              " '3. How does an LLM-powered system differ from other autonomous agent systems in terms of components and functionality?']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer Recursively"
      ],
      "metadata": {
        "id": "CUebh3shboIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt\n",
        "template = \"\"\"Here is the question you need to answer:\n",
        "\\n --- \\n {question} \\n --- \\n\n",
        "\n",
        "Here is any available background question + answer pairs:\n",
        "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
        "\n",
        "Here is additional context relevant to the question:\n",
        "\\n --- \\n {context} \\n --- \\n\n",
        "\n",
        "Use the above context and any background question + answer pairs to answer\n",
        "the question: \\n {question}\n",
        "\"\"\"\n",
        "\n",
        "decomposition_prompt = ChatPromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "7nUBUsRbbfWe"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def format_qa_pairs(question, answer):\n",
        "    \"\"\"Format Q and A pair\"\"\"\n",
        "\n",
        "    formatted_string = \"\"\n",
        "    formatted_string += f\"Question: {question}\\n Answer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "\n",
        "# llm\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-3.5-turbo\", temperature=0\n",
        ")\n",
        "\n",
        "q_a_pairs = \"\"\n",
        "for q in questions:\n",
        "    rag_chain = (\n",
        "        {\n",
        "            \"context\": itemgetter(\"question\") | retriever,\n",
        "            \"question\": itemgetter(\"question\"),\n",
        "            \"q_a_pairs\": itemgetter(\"q_a_pairs\")\n",
        "        }\n",
        "        | decomposition_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    answer = rag_chain.invoke(\n",
        "        {\n",
        "            \"question\": q,\n",
        "            \"q_a_pairs\": q_a_pairs\n",
        "        }\n",
        "    )\n",
        "    q_a_pairs += \"\\n--\\n\" + format_qa_pairs(q, answer)"
      ],
      "metadata": {
        "id": "b5LbJM3Cc3F8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "nE-DcjOJeKUA",
        "outputId": "6d6c1413-9e09-47e6-a819-dfa2b0552b8c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"In an LLM-powered system, LLM technology serves as the agent's brain, enabling it to understand and generate human language for communication and decision-making. This is a key component that sets it apart from other autonomous agent systems. Additionally, LLM-powered systems also include components such as planning mechanisms for breaking down complicated tasks into smaller steps, self-reflection capabilities for iterative improvement, memory mechanisms for storing and retrieving past experiences, and interaction with other agents in a coordinated manner.\\n\\nFurthermore, LLM-powered systems have been shown to handle tasks like scientific discovery, autonomous design, planning, and performance of complex experiments. They can browse the internet, read documentation, execute code, and interact with APIs to carry out various tasks autonomously. This level of functionality and versatility in handling complex tasks sets LLM-powered systems apart from other autonomous agent systems.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer individually"
      ],
      "metadata": {
        "id": "w2UoZucNjK3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer each sub-question individually\n",
        "from langchain import hub\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# RAG Prompt\n",
        "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def retrieve_and_rag(question, prompt_rag, sub_question_generator_chain):\n",
        "    \"\"\"RAG on each sub-question\"\"\"\n",
        "\n",
        "    # Use our decomposition\n",
        "    sub_questions = sub_question_generator_chain.invoke(\n",
        "        {\n",
        "            \"question\": question\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Initialize a list to hold RAG chain results\n",
        "    rag_results = []\n",
        "\n",
        "    for sub_question in sub_questions:\n",
        "        # Retrieve documents for each sub-question\n",
        "        retrieved_docs = retriever.get_relevant_documents(sub_question)\n",
        "\n",
        "        # Use retrieved documents and sub-question in RAG chain\n",
        "        answer = (\n",
        "            prompt_rag\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        ).invoke(\n",
        "            {\n",
        "                \"context\": retrieved_docs,\n",
        "                \"question\": sub_question\n",
        "            }\n",
        "        )\n",
        "        rag_results.append(answer)\n",
        "\n",
        "    return rag_results, sub_questions\n",
        "\n",
        "\n",
        "# Wrap the retrieval and RAG process in a RunnableLambda for integration\n",
        "# into a chain\n",
        "answers, questions = retrieve_and_rag(\n",
        "    question=question,\n",
        "    prompt_rag=prompt_rag,\n",
        "    sub_question_generator_chain=generate_queries_decomposition,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atc_JBh6eMfY",
        "outputId": "e91d09a1-8590-4f74-e49d-3e83d06e7f76"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-0d32ae149227>:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(sub_question)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def format_qa_pairs(questions, answers):\n",
        "    \"\"\"Format Q and A pairs\"\"\"\n",
        "    formatted_string = \"\"\n",
        "\n",
        "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
        "        formatted_string += f\"Question {i}: {question}\\n Answer: {answer}\\n\\n\"\n",
        "    return formatted_string.strip()\n",
        "\n",
        "context = format_qa_pairs(questions, answers)\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"Here is a set of Q+A pairs:\n",
        "{context}\n",
        "\n",
        "Use these to synthesize an answer to the question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "final_rag_chain.invoke(\n",
        "    {\n",
        "        \"context\": context,\n",
        "        \"question\": question\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 172
        },
        "id": "lpPkfpc-lSmu",
        "outputId": "236bc397-5b10-4bcb-8981-1675b87a9fde"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The main components of an LLM-powered autonomous agent system include LLM technology, which functions as the brain of the system, along with key components such as planning, task decomposition, self-reflection, memory, and interaction mechanisms. These components work together to enable agents to plan and execute complex tasks, learn from past experiences, and interact with other agents and external sources. By combining LLM technology with these key components, autonomous agents can enhance their performance in various tasks such as scientific discovery, autonomous design, and executing actions like browsing the internet and calling APIs.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step Back"
      ],
      "metadata": {
        "id": "Od3I0z6pomtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Few Shot Examples\n",
        "from langchain_core.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "examples = [\n",
        "    {\n",
        "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "        \"output\": \"What can the members of the Police do?\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Jan Sindel's was born in what country?\",\n",
        "        \"output\": \"What is Jan Sindel's personal history?\"\n",
        "    },\n",
        "]\n",
        "# We now transform these to example messages\n",
        "example_messages = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")\n",
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt=example_messages,\n",
        "    examples=examples,\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"Your are an expert at world knowledge. Your task is to step back\n",
        "            and paraphrase a question to amore generic step-back question, which\n",
        "            is easier to answer. Here are few examples:\n",
        "            \"\"\"\n",
        "        ),\n",
        "        # Few shot examples\n",
        "        few_shot_prompt,\n",
        "        # New question\n",
        "        (\"user\", \"{question}\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "iNF0MDvLmO-s"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_queries_step_back = (\n",
        "    prompt\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "generate_queries_step_back.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        },
        "id": "vy83ZphBq1Id",
        "outputId": "83a44c9e-6370-4bcc-d5a1-b53513c145e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is task decomposition in general?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Response prompt\n",
        "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going\n",
        "to ask you a question. Your response should be comprehensive and not contradicted\n",
        "with the following context if they are relevant.\n",
        "\n",
        "# {normal_context}\n",
        "# {step_back_context}\n",
        "\n",
        "# Original Question: {question}\n",
        "# Answer:\n",
        "\"\"\"\n",
        "\n",
        "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
        "\n",
        "chain = (\n",
        "    {\n",
        "        # Retrieve context using the normal question\n",
        "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
        "        # Retrieve context using the step-back question\n",
        "        \"step_back_context\": generate_queries_step_back | retriever,\n",
        "        # Pass on the question\n",
        "        \"question\": lambda x: x[\"question\"],\n",
        "    }\n",
        "    | response_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "bFhqbDn0rMNG",
        "outputId": "3c4622fa-8635-4553-9c51-d1133f7dd702"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition for LLM agents refers to the process of breaking down large tasks into smaller, more manageable subgoals. This allows the agent to efficiently handle complex tasks by dividing them into smaller, more easily achievable steps. \\n\\nIn the context of LLM-powered autonomous agent systems, task decomposition is a crucial component of the overall system. The system comprises of four stages, with the first stage being task planning. In this stage, the LLM acts as the brain of the system and parses user requests into multiple tasks. Each task is associated with four attributes: task type, ID, dependencies, and arguments. The LLM uses few-shot examples to guide task parsing and planning.\\n\\nTask decomposition can be achieved in several ways within the LLM agent system. One method is through simple prompting, where the LLM is asked questions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ.\" Another method is by providing task-specific instructions, such as asking the LLM to \"Write a story outline\" for writing a novel. Additionally, task decomposition can also involve human inputs to further refine the subgoals and tasks.\\n\\nFurthermore, task decomposition for LLM agents also involves model selection. The LLM distributes tasks to expert models by framing the request as a multiple-choice question and presenting a list of models to choose from. Due to the limited context length, task type-based filtration is necessary to ensure the most appropriate model is selected for each task.\\n\\nOverall, task decomposition for LLM agents is a critical aspect of their functioning, enabling them to effectively break down complex tasks into smaller, more manageable subgoals for efficient task handling and completion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HyDE (Hypothetical Documents)"
      ],
      "metadata": {
        "id": "DbkOpAJhtFUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# HyDE document generation\n",
        "template = \"\"\"Please write a scientific paper passage to answer the question\n",
        "Question: {question}\n",
        "Passage:\n",
        "\"\"\"\n",
        "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "generate_docs_for_retrieval = (\n",
        "    prompt_hyde\n",
        "    | ChatOpenAI(temperature=0)\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Run\n",
        "question = \"What is task decomposition for LLM agents?\"\n",
        "generate_docs_for_retrieval.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "mop6NJpgsbfL",
        "outputId": "9cbb89fc-0940-465b-b140-0f59f1397c4b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Task decomposition is a crucial concept in the field of reinforcement learning for Large Language Models (LLMs). LLM agents are complex models that are trained to perform a wide range of natural language processing tasks, such as text generation, question answering, and language translation. Task decomposition refers to the process of breaking down a complex task into smaller, more manageable sub-tasks that can be tackled individually by the agent.\\n\\nBy decomposing a task into smaller sub-tasks, LLM agents can effectively leverage their capabilities to solve complex problems. This approach allows the agent to focus on solving each sub-task independently, which can lead to more efficient and effective problem-solving. Additionally, task decomposition can help improve the interpretability and generalization of the agent's learned policies, as it allows for a more structured and modular approach to learning.\\n\\nOverall, task decomposition plays a crucial role in enabling LLM agents to effectively tackle complex natural language processing tasks by breaking them down into smaller, more manageable components. This approach can lead to improved performance, efficiency, and interpretability of the agent's learned policies.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve\n",
        "retrieval_chain = generate_docs_for_retrieval | retriever\n",
        "retrieved_docs = retrieval_chain.invoke(\n",
        "    {\n",
        "        \"question\": question\n",
        "    }\n",
        ")\n",
        "retrieved_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE3Phs7aw-Ii",
        "outputId": "d8bfe1e2-95fc-41e6-a765-50834f8e3ae4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
              " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Challenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.')]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RAG\n",
        "template = \"\"\"Answer the following question based on this context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "final_rag_chain = (\n",
        "    prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "final_rag_chain.invoke(\n",
        "    {\n",
        "        \"context\": retrieved_docs,\n",
        "        \"question\": question\n",
        "    }\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "NemQGNp4xoLh",
        "outputId": "5dde3b5c-891d-411f-aa2e-33be62772a0f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Task decomposition for LLM agents involves breaking down large tasks into smaller, manageable subgoals using simple prompting, task-specific instructions, or human inputs. This process enables efficient handling of complex tasks by the agent.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}